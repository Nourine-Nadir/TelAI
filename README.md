# Federated Anomaly Detection with Transformer

A PyTorch implementation of federated learning for network anomaly detection using Transformer architecture on the UNSW-NB15 dataset.

![Python](https://img.shields.io/badge/Python-3.8%2B-blue)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-red)
![Federated Learning](https://img.shields.io/badge/Federated%20Learning-Enabled-green)
![License](https://img.shields.io/badge/License-MIT-yellow)

## ğŸ“– Overview

This project implements a federated learning framework for network intrusion detection using a Transformer-based model. The system can operate in both centralized and federated learning modes, allowing for privacy-preserving collaborative training across multiple data silos without sharing raw data.

## âœ¨ Features

- **Transformer Architecture**: Utilizes self-attention mechanisms for effective anomaly detection
- **Federated Learning**: Implements FedAvg algorithm for privacy-preserving training
- **Multiple Clients**: Supports training across distributed data sources
- **UNSW-NB15 Dataset**: Preprocessing and integration with the benchmark network intrusion dataset
- **Flexible Configuration**: Easy switching between centralized and federated modes
- **Comprehensive Evaluation**: Includes accuracy, confusion matrix, and classification reports

## ğŸ—ï¸ Architecture

### Model Architecture
```
AnomalyTransformer
â”œâ”€â”€ Input Projection (Linear)] 
â”œâ”€â”€ Transformer Encoder (Multi-head Attention)
â”œâ”€â”€ Temporal Pooling (Mean)
â””â”€â”€ Classifier (MLP)
```

### Federated Learning Flow
1. **Initialization**: Global model is initialized and distributed to clients
2. **Local Training**: Each client trains on its local data
3. **Aggregation**: Server performs federated averaging (FedAvg)
4. **Distribution**: Updated global model is sent back to clients
5. **Evaluation**: Periodic validation of global model performance

## ğŸ“¦ Installation

### Prerequisites
- Python 3.8+
- PyTorch 2.0+
- CUDA-enabled GPU (recommended)

### Install Dependencies

```bash
git clone https://github.com/your-username/federated-anomaly-detection.git
cd federated-anomaly-detection

pip install -r requirements.txt
```

## ğŸ“Š Dataset Setup
```Data
â””â”€â”€ Data
    â””â”€â”€ UNSW-NB15
        â”œâ”€â”€ UNSW_NB15_training-set.csv
        â””â”€â”€ UNSW_NB15_testing-set.csv
â””â”€â”€ src
    â”œâ”€â”€ EDA.ipynb          # Exploratory data analysis notebook
    â”œâ”€â”€ models.py          # Transformer model definition
    â”œâ”€â”€ utils.py           # Data preprocessing and dataset classes
    â”œâ”€â”€ train.py           # Training utilities
    â”œâ”€â”€ test.py            # Evaluation and metrics
    â”œâ”€â”€ federated.py       # Federated learning implementation
    â”œâ”€â”€ main.py           # Main entry point
â”œâ”€â”€ requirements.txt   # Python dependencies
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE
```

## ğŸš€ Usage

### Centralized Training
```
python main.py
# Choose option 1 when prompted
```
### Federated Training

```
python main.py
# Choose option 2 when prompted
```

#### Custom Configuration
```
# Number of clients
num_clients = 5

# Training parameters
num_rounds = 50
epochs_per_client = 3
learning_rate = 1e-4
batch_size = 512
sequence_length = 5
```
#### Sample Output
```
Federated Rounds: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [03:11<00:00,  3.82s/it]
Final Evaluation:

Validation Accuracy: 0.9247747233945477

Classification Report:
               precision    recall  f1-score   support

           0       0.80      0.97      0.88      4794
           1       0.99      0.91      0.95     12740

    accuracy                           0.92     17534
   macro avg       0.89      0.94      0.91     17534
weighted avg       0.94      0.92      0.93     17534
```